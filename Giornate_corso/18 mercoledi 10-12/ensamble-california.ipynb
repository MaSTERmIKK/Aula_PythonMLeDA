{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfc9cb9f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Che cos'è lo Stacking (Stacked Generalization)?\n",
    "\n",
    "Immagina di avere un problema difficile e di chiedere consiglio a tre esperti diversi:\n",
    "\n",
    "1.  Un matematico (XGBoost)\n",
    "2.  Un fisico (CatBoost)\n",
    "3.  Un ingegnere (LightGBM)\n",
    "\n",
    "Ognuno ti dà una risposta leggermente diversa.\n",
    "Invece di fare una media semplice (che sarebbe il *Voting* o *Averaging*), assumi un **Manager** (il Meta-Learner). Il Manager ha studiato come si sono comportati gli esperti in passato e sa che: *\"Quando il matematico dice X e il fisico dice Y, di solito ha ragione il fisico, ma l'ingegnere corregge l'errore sui casi limite\".*\n",
    "\n",
    "Il Manager impara a **pesare** le previsioni degli esperti per dare la risposta finale ottimale.\n",
    "\n",
    "### Come funziona \n",
    "\n",
    "Nello script, lo stacking avviene in due livelli (Level 0 e Level 1).\n",
    "\n",
    "#### Livello 0: I \"Base Learners\" (Gli Esperti)\n",
    "\n",
    "Questi sono i modelli forti che hai ottimizzato con Optuna. Nel tuo codice sono definiti qui:\n",
    "\n",
    "```python\n",
    "estimators = [\n",
    "    ('xgb', XGBRegressor(**best_xgb_params)),\n",
    "    ('cat', CatBoostRegressor(**best_cat_params)),\n",
    "    ('lgbm', LGBMRegressor(**best_lgbm_params))\n",
    "]\n",
    "```\n",
    "\n",
    "Questi modelli guardano i dati originali (`X_train_full`) e cercano di predire il prezzo delle case (`y`). Essendo tre varianti di Gradient Boosting, sono tutti molto potenti, ma faranno errori leggermente diversi su case diverse.\n",
    "\n",
    "#### Livello 1: Il \"Meta Learner\" (Il Manager)\n",
    "\n",
    "Questo è il modello che prende le decisioni finali. Nel tuo codice è:\n",
    "\n",
    "```python\n",
    "final_estimator = RidgeCV()\n",
    "```\n",
    "\n",
    "Viene scelta una **Ridge Regression** (regressione lineare con regolarizzazione L2). Questa è una scelta **standard** per lo stacking.\n",
    "\n",
    "**Perché RidgeCV?**\n",
    "I modelli di base (XGB, Cat, LGBM) sono molto complessi e non lineari. Il Meta-Learner non deve imparare di nuovo i pattern complessi dei dati; deve solo imparare **come combinare linearmente** le predizioni dei tre modelli sopra. Una regressione semplice evita che il Meta-Learner vada in overfitting sulle predizioni.\n",
    "\n",
    "### Il processo \"Sotto il cofano\" (Cross-Validation)\n",
    "\n",
    "Nel codice:\n",
    "\n",
    "```python\n",
    "stacking = StackingRegressor(..., cv=5, ...)\n",
    "```\n",
    "\n",
    "Questo `cv=5` è cruciale. Ecco cosa succede quando lanci `stacking.fit`:\n",
    "\n",
    "1.  **Generazione delle Meta-Feature (Out-of-Fold Predictions):**\n",
    "\n",
    "      * Il dataset di train viene diviso in 5 parti (folds).\n",
    "      * I modelli base (XGB, Cat, LGBM) vengono addestrati su 4 parti e fanno previsioni sulla 5ª parte (che non hanno mai visto).\n",
    "      * Si ripete per tutte le 5 parti.\n",
    "      * Alla fine, per ogni riga del tuo dataset di train, hai 3 previsioni (una per modello) che sono state generate senza \"vedere\" quel dato durante il training. Queste 3 previsioni diventano le nuove colonne (Meta-Features) per il Meta-Learner.\n",
    "\n",
    "2.  **Addestramento Meta-Learner:**\n",
    "\n",
    "      * Il `RidgeCV` viene addestrato su queste 3 nuove colonne per predire il target reale. Impara i coefficienti (i pesi) da dare a ciascun modello.\n",
    "\n",
    "3.  **Retraining Finale:**\n",
    "\n",
    "      * Una volta che il `RidgeCV` è addestrato, i modelli base (XGB, Cat, LGBM) vengono ri-addestrati su **tutto** il dataset `X_train_full` per essere pronti a fare previsioni sui nuovi dati (`X_test`).\n",
    "\n",
    "### Perché questo approccio è vincente\n",
    "\n",
    "1.  **Diversity:** Anche usando tre librerie di Boosting (che sono simili), hanno implementazioni diverse (CatBoost gestisce meglio le categoriche/numeriche, XGBoost è aggressivo, LightGBM è veloce e usa leaf-wise growth). I loro errori sono spesso decorrelati.\n",
    "2.  **Robustezza:** La RidgeCV come meta-learner \"liscia\" le predizioni. Se XGBoost impazzisce su una casa e predice un valore assurdo, ma CatBoost e LightGBM sono stabili, la Ridge ridurrà l'impatto dell'errore di XGBoost.\n",
    "3.  **Ottimizzazione Precedente:** Si passano i `best_params` da Optuna. Stai facendo stacking di modelli *già ottimizzati*, non di modelli a caso. Questo massimizza il guadagno.\n",
    "\n",
    "### Sintesi dei Risultati\n",
    "\n",
    "Lo stacking quasi sempre batte il miglior modello singolo, solitamente riducendo l'RMSE di un ulteriore 0.5% - 1.5%. Nel tuo output finale vedrai che l'RMSE dello stacking sarà probabilmente inferiore al \"BEST VAL RMSE\" che hai stampato per i singoli modelli Optuna.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5bcca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import warnings\n",
    "import itertools\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Dataset e Preprocessing\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Modelli\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor, early_stopping, log_evaluation\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# Configurazione\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. CARICAMENTO E PULIZIA DATI\n",
    "# ---------------------------------------------------------\n",
    "print(\"1. Caricamento e Pulizia Dati...\")\n",
    "start_global = time.time()\n",
    "\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name=\"MedHouseVal\")\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n",
    "# Rimuovi il cap a 5.0 (valore massimo del dataset originale che crea distorsioni)\n",
    "df = df[df['MedHouseVal'] < 5.0]\n",
    "\n",
    "def remove_outliers_iqr(df, columns):\n",
    "    df_clean = df.copy()\n",
    "    indices_to_drop = []\n",
    "    for col in columns:\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = df_clean[(df_clean[col] < Q1 - 2.0*IQR) | (df_clean[col] > Q3 + 2.0*IQR)].index\n",
    "        indices_to_drop.extend(outliers)\n",
    "    return df_clean.drop(list(set(indices_to_drop)))\n",
    "\n",
    "cols_clean = ['AveRooms', 'AveBedrms', 'AveOccup', 'MedInc']\n",
    "df = remove_outliers_iqr(df, cols_clean)\n",
    "print(f\"   -> Dataset pulito: {df.shape}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. FEATURE ENGINEERING MASSIVA\n",
    "# ---------------------------------------------------------\n",
    "print(\"2. Generazione Feature 'Combo' (Multiplicazioni, Divisioni, Log)...\")\n",
    "\n",
    "def generate_comprehensive_features(df_input, cols_to_combine):\n",
    "    df_eng = df_input.copy()\n",
    "    math_cols = [c for c in cols_to_combine if c not in ['Latitude', 'Longitude', 'Geo_Cluster']]\n",
    "    \n",
    "    print(f\"   -> Generazione combinazioni su {len(math_cols)} colonne base...\")\n",
    "\n",
    "    # A. LOGARITMI\n",
    "    for col in math_cols:\n",
    "        if df_eng[col].min() >= 0:\n",
    "            df_eng[f'LOG_{col}'] = np.log1p(df_eng[col])\n",
    "\n",
    "    # B. MOLTIPLICAZIONI\n",
    "    for col1, col2 in itertools.combinations(math_cols, 2):\n",
    "        col_name = f'MULT_{col1}_x_{col2}'\n",
    "        df_eng[col_name] = df_eng[col1] * df_eng[col2]\n",
    "\n",
    "    # C. DIVISIONI / RAPPORTI\n",
    "    for col1, col2 in itertools.permutations(math_cols, 2):\n",
    "        col_name = f'RATIO_{col1}_div_{col2}'\n",
    "        df_eng[col_name] = df_eng[col1] / (df_eng[col2] + 1e-5)\n",
    "\n",
    "    return df_eng\n",
    "\n",
    "# --- Geo Features Base ---\n",
    "sf_coords = (37.7749, -122.4194)\n",
    "la_coords = (34.0522, -118.2437)\n",
    "df['Dist_SF'] = np.sqrt((df['Latitude'] - sf_coords[0])**2 + (df['Longitude'] - sf_coords[1])**2)\n",
    "df['Dist_LA'] = np.sqrt((df['Latitude'] - la_coords[0])**2 + (df['Longitude'] - la_coords[1])**2)\n",
    "\n",
    "coords = df[['Latitude', 'Longitude']]\n",
    "# KMeans (Cluster geografici)\n",
    "kmeans = KMeans(n_clusters=15, random_state=42, n_init=10)\n",
    "df['Geo_Cluster'] = kmeans.fit_predict(StandardScaler().fit_transform(coords))\n",
    "\n",
    "X = df.drop('MedHouseVal', axis=1)\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "cols_for_math = [c for c in X.columns if c != 'Geo_Cluster']\n",
    "X_full = generate_comprehensive_features(X, cols_for_math)\n",
    "\n",
    "# Pulizia inf/nan generati dalle divisioni\n",
    "X_full.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_full.fillna(0, inplace=True)\n",
    "\n",
    "print(f\"   -> Totale Feature Generate: {X_full.shape[1]}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. IL GIUDICE: SELEZIONE FEATURE CON XGBOOST GPU\n",
    "# ---------------------------------------------------------\n",
    "print(\"3. Selezione Feature tramite GPU...\")\n",
    "\n",
    "scaler = RobustScaler()\n",
    "# Nota: La selezione feature non necessita di early stopping complesso\n",
    "selector_model = XGBRegressor(\n",
    "    n_estimators=500, max_depth=8, learning_rate=0.05,\n",
    "    tree_method='gpu_hist', \n",
    "    device='gpu',\n",
    "    n_jobs=1,\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "selector_model.fit(X_full, y)\n",
    "\n",
    "selection = SelectFromModel(selector_model, prefit=True, threshold='1.25*median')\n",
    "\n",
    "X_selected = X_full.loc[:, selection.get_support()]\n",
    "print(f\"   -> Feature Sopravvissute: {X_selected.shape[1]}\")\n",
    "\n",
    "selected_names = X_full.columns[selection.get_support()]\n",
    "print(f\"   -> Esempio Top Feature scelte: {list(selected_names[:5])}...\")\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# =========================================================\n",
    "# === SPLIT PER OPTUNA VELOCE (HOLD-OUT) ===\n",
    "# =========================================================\n",
    "X_opt_train, X_opt_val, y_opt_train, y_opt_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"\\nSet Ottimizzazione: Train={X_opt_train.shape[0]}, Validation={X_opt_val.shape[0]}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. OPTUNA SUPER VELOCE (Aggiornato per nuove API)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n4. Tuning Iperparametri VELOCE (Hold-out validation con GPU)...\")\n",
    "N_TRIALS = 50 \n",
    "\n",
    "# --- A. XGBoost (GPU - Aggiornato) ---\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': 4000, \n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 12),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.95),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.95),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 10),\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'device': 'gpu',\n",
    "        'n_jobs': 1,\n",
    "        'random_state': 42,\n",
    "        'eval_metric': 'rmse',\n",
    "        # FIX: early_stopping_rounds va qui nel costruttore per le nuove versioni\n",
    "        'early_stopping_rounds': 100 \n",
    "    }\n",
    "    \n",
    "    # Inizializza con early_stopping_rounds\n",
    "    model = XGBRegressor(**params)\n",
    "    \n",
    "\n",
    "    model.fit(\n",
    "        X_opt_train, y_opt_train,\n",
    "        eval_set=[(X_opt_val, y_opt_val)],\n",
    "        verbose=False # In XGBoost 2.0+ a volte verbose in fit è deprecato, ma False di solito passa\n",
    "    )\n",
    "    \n",
    "    preds = model.predict(X_opt_val)\n",
    "    return np.sqrt(mean_squared_error(y_opt_val, preds))\n",
    "\n",
    "print(\" -> Optimizing XGBoost...\")\n",
    "study_xgb = optuna.create_study(direction='minimize')\n",
    "study_xgb.optimize(objective_xgb, n_trials=N_TRIALS)\n",
    "\n",
    "# --- B. CatBoost (GPU) ---\n",
    "def objective_cat(trial):\n",
    "    params = {\n",
    "        'iterations': 4000,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1),\n",
    "        'depth': trial.suggest_int('depth', 6, 13), \n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.95),\n",
    "        'task_type': 'GPU',\n",
    "        'devices': '0',\n",
    "        'verbose': 0,\n",
    "        'allow_writing_files': False,\n",
    "        'random_state': 42,\n",
    "        'eval_metric': 'RMSE'\n",
    "    }\n",
    "    model = CatBoostRegressor(**params)\n",
    "    \n",
    "    # CatBoost accetta ancora early_stopping_rounds in fit\n",
    "    model.fit(\n",
    "        X_opt_train, y_opt_train,\n",
    "        eval_set=[(X_opt_val, y_opt_val)],\n",
    "        early_stopping_rounds=100\n",
    "    )\n",
    "    \n",
    "    preds = model.predict(X_opt_val)\n",
    "    return np.sqrt(mean_squared_error(y_opt_val, preds))\n",
    "\n",
    "print(\" -> Optimizing CatBoost...\")\n",
    "study_cat = optuna.create_study(direction='minimize')\n",
    "study_cat.optimize(objective_cat, n_trials=N_TRIALS) \n",
    "\n",
    "# --- C. LightGBM (Hybrid - Aggiornato) ---\n",
    "def objective_lgbm(trial):\n",
    "    params = {\n",
    "        'n_estimators': 4000,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 30, 200),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 0.95),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 0.95),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 10),\n",
    "        'device': 'gpu',\n",
    "        'n_jobs': 1,\n",
    "        'verbosity': -1, # Usa verbosity qui invece di verbose\n",
    "        'random_state': 42,\n",
    "        'metric': 'rmse'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        model = LGBMRegressor(**params)\n",
    "        # FIX: Usa callbacks espliciti, rimuovi early_stopping_rounds dagli argomenti diretti\n",
    "        model.fit(\n",
    "            X_opt_train, y_opt_train,\n",
    "            eval_set=[(X_opt_val, y_opt_val)],\n",
    "            eval_metric='rmse',\n",
    "            callbacks=[early_stopping(stopping_rounds=100, verbose=False), log_evaluation(period=0)]\n",
    "        )\n",
    "        preds = model.predict(X_opt_val)\n",
    "        return np.sqrt(mean_squared_error(y_opt_val, preds))\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Fallback CPU se GPU non va o errore specifico\n",
    "        # print(f\"LGBM GPU Error: {e}\") \n",
    "        params['device'] = 'cpu'\n",
    "        params['n_jobs'] = -1 \n",
    "        model = LGBMRegressor(**params)\n",
    "        model.fit(\n",
    "            X_opt_train, y_opt_train,\n",
    "            eval_set=[(X_opt_val, y_opt_val)],\n",
    "            eval_metric='rmse',\n",
    "            callbacks=[early_stopping(stopping_rounds=100, verbose=False), log_evaluation(period=0)]\n",
    "        )\n",
    "        preds = model.predict(X_opt_val)\n",
    "        return np.sqrt(mean_squared_error(y_opt_val, preds))\n",
    "\n",
    "print(\" -> Optimizing LightGBM...\")\n",
    "study_lgbm = optuna.create_study(direction='minimize')\n",
    "study_lgbm.optimize(objective_lgbm, n_trials=N_TRIALS)\n",
    "\n",
    "print(f\"\\nBEST VAL RMSE:\")\n",
    "print(f\"XGB: {study_xgb.best_value:.4f}\")\n",
    "print(f\"CAT: {study_cat.best_value:.4f}\")\n",
    "print(f\"LGBM: {study_lgbm.best_value:.4f}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. STACKING FINALE (Ensemble)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n5. Addestramento Stacking Ensemble Finale (su set completo)...\")\n",
    "\n",
    "# Recuperiamo i migliori parametri \n",
    "best_xgb_params = study_xgb.best_params\n",
    "# Assicuriamoci che non ci siano parametri di fit residui\n",
    "best_xgb_params.update({\n",
    "    'tree_method': 'gpu_hist', 'device': 'gpu', 'n_jobs': 1, \n",
    "    'random_state': 42, 'early_stopping_rounds': None # Disabilita early stopping per il fit finale\n",
    "})\n",
    "\n",
    "best_cat_params = study_cat.best_params\n",
    "best_cat_params.update({\n",
    "    'task_type': 'GPU', 'devices': '0', 'verbose': 0, \n",
    "    'allow_writing_files': False, 'random_state': 42\n",
    "})\n",
    "\n",
    "best_lgbm_params = study_lgbm.best_params\n",
    "if study_lgbm.best_params.get('device') == 'cpu':\n",
    "    best_lgbm_params.update({'n_jobs': -1, 'verbosity': -1, 'random_state': 42})\n",
    "else:\n",
    "    best_lgbm_params.update({'device': 'gpu', 'n_jobs': 1, 'verbosity': -1, 'random_state': 42})\n",
    "\n",
    "estimators = [\n",
    "    ('xgb', XGBRegressor(**best_xgb_params)),\n",
    "    ('cat', CatBoostRegressor(**best_cat_params)),\n",
    "    ('lgbm', LGBMRegressor(**best_lgbm_params))\n",
    "]\n",
    "\n",
    "# Stacking Regressor non supporta nativamente early stopping nei sotto-modelli\n",
    "# durante il cross-validation interno in modo semplice, quindi usiamo n_estimators ottimizzati\n",
    "# (Nota: idealmente dovresti settare n_estimators al valore ottimo trovato durante Optuna, \n",
    "# ma qui li lasciamo alti o default per lo stacking standard)\n",
    "\n",
    "stacking = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=RidgeCV(),\n",
    "    cv=5,\n",
    "    n_jobs=1 \n",
    ")\n",
    "\n",
    "stacking.fit(X_train_full, y_train_full)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. VALUTAZIONE FINALE\n",
    "# ---------------------------------------------------------\n",
    "y_pred = stacking.predict(X_test)\n",
    "final_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "final_r2 = r2_score(y_test, y_pred)\n",
    "total_time = time.time() - start_global\n",
    "\n",
    "print(f\"\\n==========================================\")\n",
    "print(f\" RISULTATI FINALI (CODICE AGGIORNATO)\")\n",
    "print(f\"==========================================\")\n",
    "print(f\" RMSE: {final_rmse:.5f}\")\n",
    "print(f\" R^2 : {final_r2:.5f}\")\n",
    "print(f\" Tempo Totale: {total_time:.2f} sec\")\n",
    "print(f\"==========================================\")\n",
    "\n",
    "# Plot veloce importanza feature (su XGBoost)\n",
    "final_xgb = XGBRegressor(**best_xgb_params)\n",
    "final_xgb.fit(X_train_full, y_train_full)\n",
    "plt.figure(figsize=(10, 8))\n",
    "feat_importances = pd.Series(final_xgb.feature_importances_, index=X_train_full.columns)\n",
    "feat_importances.nlargest(20).plot(kind='barh')\n",
    "plt.title(\"Top 20 Feature Importanti (XGBoost GPU)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99710dea",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1. Il Primo Stadio: `SmartSoftModelSelector` (Il Cervello)\n",
    "\n",
    "Nel codice precedente si usava una `RidgeCV` che assegnava pesi fissi (es. 0.4 a XGB, 0.4 a Cat, 0.2 a LGBM).\n",
    "Qui invece si crea una **Gating Network** dinamica.\n",
    "\n",
    "* **Il Concetto:** Non tutti i modelli sono bravi ovunque. Forse XGBoost è bravissimo con le case costose, mentre CatBoost è più preciso con le case vecchie.\n",
    "* **Come funziona nel codice:**\n",
    "    1.  Addestri i 3 modelli base (\"Gli Esperti\").\n",
    "    2.  Calcoli chi ha sbagliato di meno per ogni singola casa del training set (`y_best_model_idx`).\n",
    "    3.  Addestri un classificatore (`self.selector_model`, un XGBClassifier) che prende le caratteristiche della casa e predice: *\"Quale esperto devo ascoltare per questa specifica casa?\"*.\n",
    "    4.  **Soft Voting:** Non scegli solo un vincitore. Se il selettore dice: \"80% probabilità che XGBoost abbia ragione, 20% CatBoost\", la predizione finale sarà una media pesata 80/20.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Il Secondo Stadio: `ResidualCorrectedMoE` (Il Modello dei Bias)\n",
    "\n",
    "È una tecnica potente spesso usata nelle competizioni Kaggle o nei sistemi finanziari per correggere errori sistematici.\n",
    "\n",
    "#### La Logica Matematica\n",
    "Normalmente, l'equazione è:\n",
    "$$\\text{Predizione} = f(X)$$\n",
    "Nel tuo codice, l'equazione diventa:\n",
    "$$\\text{Predizione Finale} = f_{MoE}(X) + g_{Correttore}(X)$$\n",
    "\n",
    "Dove $g(X)$ stima l'errore commesso da $f(X)$.\n",
    "\n",
    "#### Cosa fa il codice riga per riga:\n",
    "\n",
    "1.  **`base_moe_model.fit(X, y)`**:\n",
    "    Il primo stadio fa il grosso del lavoro e impara il prezzo delle case.\n",
    "\n",
    "2.  **`cross_val_predict(..., cv=5)` (Cruciale!)**:\n",
    "    Qui sta la magia. Il codice genera le predizioni sul training set usando la cross-validation.\n",
    "    * *Perché non usare `.predict()` normale?* Se usassi predict sui dati su cui hai appena addestrato, l'errore sarebbe bassissimo (overfitting). Usando `cross_val_predict`, simuli come il modello si comporta su dati **mai visti**.\n",
    "\n",
    "3.  **`residuals = y - oof_preds`**:\n",
    "    Calcoli l'errore.\n",
    "    * Se `residual` è positivo (+10k), significa che il modello base ha **sottostimato** il prezzo.\n",
    "    * Se `residual` è negativo (-10k), ha **sovrastimato**.\n",
    "\n",
    "4.  **`corrector_model.fit(X, residuals)`**:\n",
    "    Addestri un nuovo modello (CatBoost leggero) non a predire il prezzo, ma a predire **di quanto sbaglierà il primo modello**.\n",
    "    Il correttore impara i **Bias Sistematici**.\n",
    "    * *Esempio:* \"Ogni volta che la casa è a San Francisco (`Dist_SF` basso) e ha 2 bagni, il modello principale tende a sottostimare di 15.000$\". Il correttore impara questo pattern e aggiungerà automaticamente 15.000$.\n",
    "\n",
    "\n",
    "\n",
    "### Perché questo approccio è rischioso ma geniale?\n",
    "\n",
    "1.  **Geniale:** Rimuovi le distorsioni strutturali. Se i tuoi modelli base sono tutti \"prudenti\" sui prezzi alti, il correttore imparerà ad essere \"aggressivo\" per compensare.\n",
    "2.  **Rischioso (Overfitting):** C'è il rischio che il correttore impari il \"rumore\" invece del bias reale.\n",
    "    * *La tua protezione:* Nel codice vedo che hai configurato il `corrector_model` con parametri conservativi (`depth=6`, `l2_leaf_reg=5`, `iterations=500`). Questo è corretto: il correttore deve essere \"debole\" per catturare solo i trend generali dell'errore e non memorizzare il dataset.\n",
    "\n",
    "### Sintesi Grafica della tua Pipeline\n",
    "\n",
    "Possiamo riassumere il flusso dei dati nel tuo script così:\n",
    "\n",
    "1.  **Input Dati**\n",
    "2.  **Livello 1 (Esperti):** XGB + Cat + LGBM generano 3 opinioni.\n",
    "3.  **Livello 1.5 (Gating):** Il Classificatore decide quanto fidarsi di ognuno -> Esce `Base Prediction`.\n",
    "4.  **Livello 2 (Correttore):** Il CatBoost dei Residui guarda i dati e dice: \"Attenzione, qui stiamo sottostimando\". -> Esce `Correction`.\n",
    "5.  **Output Finale:** `Base Prediction` + `Correction`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa5125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import warnings\n",
    "import itertools\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Dataset e Preprocessing\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "# Modelli\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from lightgbm import LGBMRegressor, early_stopping, log_evaluation\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Configurazione\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. CARICAMENTO E PULIZIA DATI\n",
    "# ---------------------------------------------------------\n",
    "print(\"1. Caricamento e Pulizia Dati...\")\n",
    "start_global = time.time()\n",
    "\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name=\"MedHouseVal\")\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n",
    "# Rimuoviamo il cap a 5.0\n",
    "df = df[df['MedHouseVal'] < 5.0]\n",
    "\n",
    "def remove_outliers_iqr(df, columns):\n",
    "    df_clean = df.copy()\n",
    "    indices_to_drop = []\n",
    "    for col in columns:\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = df_clean[(df_clean[col] < Q1 - 2.0*IQR) | (df_clean[col] > Q3 + 2.0*IQR)].index\n",
    "        indices_to_drop.extend(outliers)\n",
    "    return df_clean.drop(list(set(indices_to_drop)))\n",
    "\n",
    "cols_clean = ['AveRooms', 'AveBedrms', 'AveOccup', 'MedInc']\n",
    "df = remove_outliers_iqr(df, cols_clean)\n",
    "print(f\"   -> Dataset pulito: {df.shape}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. FEATURE ENGINEERING MASSIVA\n",
    "# ---------------------------------------------------------\n",
    "print(\"2. Generazione Feature 'Combo'...\")\n",
    "\n",
    "def generate_comprehensive_features(df_input, cols_to_combine):\n",
    "    df_eng = df_input.copy()\n",
    "    math_cols = [c for c in cols_to_combine if c not in ['Latitude', 'Longitude', 'Geo_Cluster']]\n",
    "    \n",
    "    # Trucco Rotazione Coordinate per Alberi\n",
    "    df_eng['Rot_45_LatLon'] = df_eng['Latitude'] + df_eng['Longitude']\n",
    "    df_eng['Rot_N45_LatLon'] = df_eng['Latitude'] - df_eng['Longitude']\n",
    "\n",
    "    # A. LOGARITMI\n",
    "    for col in math_cols:\n",
    "        if df_eng[col].min() >= 0:\n",
    "            df_eng[f'LOG_{col}'] = np.log1p(df_eng[col])\n",
    "\n",
    "    # B. MOLTIPLICAZIONI\n",
    "    for col1, col2 in itertools.combinations(math_cols, 2):\n",
    "        col_name = f'MULT_{col1}_x_{col2}'\n",
    "        df_eng[col_name] = df_eng[col1] * df_eng[col2]\n",
    "\n",
    "    # C. DIVISIONI\n",
    "    for col1, col2 in itertools.permutations(math_cols, 2):\n",
    "        col_name = f'RATIO_{col1}_div_{col2}'\n",
    "        df_eng[col_name] = df_eng[col1] / (df_eng[col2] + 1e-5)\n",
    "\n",
    "    return df_eng\n",
    "\n",
    "# Geo Features Base\n",
    "sf_coords = (37.7749, -122.4194)\n",
    "la_coords = (34.0522, -118.2437)\n",
    "df['Dist_SF'] = np.sqrt((df['Latitude'] - sf_coords[0])**2 + (df['Longitude'] - sf_coords[1])**2)\n",
    "df['Dist_LA'] = np.sqrt((df['Latitude'] - la_coords[0])**2 + (df['Longitude'] - la_coords[1])**2)\n",
    "\n",
    "coords = df[['Latitude', 'Longitude']]\n",
    "kmeans = KMeans(n_clusters=15, random_state=42, n_init=10)\n",
    "df['Geo_Cluster'] = kmeans.fit_predict(StandardScaler().fit_transform(coords))\n",
    "\n",
    "X = df.drop('MedHouseVal', axis=1)\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "cols_for_math = [c for c in X.columns if c != 'Geo_Cluster']\n",
    "X_full = generate_comprehensive_features(X, cols_for_math)\n",
    "X_full.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_full.fillna(0, inplace=True)\n",
    "print(f\"   -> Totale Feature Generate: {X_full.shape[1]}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. SELEZIONE FEATURE CON XGBOOST GPU\n",
    "# ---------------------------------------------------------\n",
    "print(\"3. Selezione Feature tramite GPU...\")\n",
    "scaler = RobustScaler()\n",
    "# Nota: fit non richiede scaling per alberi, ma utile averlo per prassi\n",
    "X_scaled = scaler.fit_transform(X_full)\n",
    "\n",
    "selector_model = XGBRegressor(\n",
    "    n_estimators=500, max_depth=8, learning_rate=0.05,\n",
    "    tree_method='hist', device='gpu', n_jobs=1, random_state=42\n",
    ")\n",
    "selector_model.fit(X_full, y)\n",
    "selection = SelectFromModel(selector_model, prefit=True, threshold='1.25*median')\n",
    "\n",
    "X_selected = X_full.loc[:, selection.get_support()]\n",
    "print(f\"   -> Feature Sopravvissute: {X_selected.shape[1]}\")\n",
    "\n",
    "# Split Principale (Train / Test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. OPTUNA REALE CON EARLY STOPPING CORRETTO\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n4. Tuning Iperparametri con Optuna e Early Stopping...\")\n",
    "\n",
    "# Creiamo un set di validazione SOLO per Optuna per monitorare l'early stopping\n",
    "X_opt_train, X_opt_val, y_opt_train, y_opt_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "N_TRIALS = 20  # Numero di trial (aumentare per risultati migliori)\n",
    "\n",
    "# --- A. XGBoost Optimization ---\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': 4000,  # Alto, tanto ferma l'early stopping\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.95),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.95),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10.0, log=True),\n",
    "        'tree_method': 'hist',\n",
    "        'device': 'gpu',\n",
    "        'n_jobs': 1,\n",
    "        'random_state': 42,\n",
    "        # Early Stopping nel costruttore per versioni recenti (o gestito in fit)\n",
    "        'early_stopping_rounds': 100\n",
    "    }\n",
    "    \n",
    "    model = XGBRegressor(**params)\n",
    "    \n",
    "    # Verbose=False sopprime i log, eval_set serve per l'early stopping\n",
    "    model.fit(\n",
    "        X_opt_train, y_opt_train,\n",
    "        eval_set=[(X_opt_val, y_opt_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    preds = model.predict(X_opt_val)\n",
    "    return np.sqrt(mean_squared_error(y_opt_val, preds))\n",
    "\n",
    "print(\"   -> Optimizing XGBoost...\")\n",
    "study_xgb = optuna.create_study(direction='minimize')\n",
    "study_xgb.optimize(objective_xgb, n_trials=N_TRIALS)\n",
    "\n",
    "# --- B. CatBoost Optimization ---\n",
    "def objective_cat(trial):\n",
    "    params = {\n",
    "        'iterations': 4000,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
    "        'depth': trial.suggest_int('depth', 6, 12),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.95),\n",
    "        'task_type': 'GPU',\n",
    "        'devices': '0',\n",
    "        'verbose': 0,\n",
    "        'allow_writing_files': False,\n",
    "        'random_state': 42,\n",
    "        'eval_metric': 'RMSE'\n",
    "    }\n",
    "    \n",
    "    model = CatBoostRegressor(**params)\n",
    "    \n",
    "    model.fit(\n",
    "        X_opt_train, y_opt_train,\n",
    "        eval_set=[(X_opt_val, y_opt_val)],\n",
    "        early_stopping_rounds=100\n",
    "    )\n",
    "    \n",
    "    preds = model.predict(X_opt_val)\n",
    "    return np.sqrt(mean_squared_error(y_opt_val, preds))\n",
    "\n",
    "print(\"   -> Optimizing CatBoost...\")\n",
    "study_cat = optuna.create_study(direction='minimize')\n",
    "study_cat.optimize(objective_cat, n_trials=N_TRIALS)\n",
    "\n",
    "# --- C. LightGBM Optimization (New Callback API) ---\n",
    "def objective_lgbm(trial):\n",
    "    params = {\n",
    "        'n_estimators': 4000,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 30, 200),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 0.95),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 0.95),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10),\n",
    "        'device': 'gpu', # Assicurati di avere LGBM compilato per GPU, altrimenti usa 'cpu'\n",
    "        'n_jobs': 1,\n",
    "        'verbosity': -1,\n",
    "        'random_state': 42,\n",
    "        'metric': 'rmse'\n",
    "    }\n",
    "    \n",
    "    model = LGBMRegressor(**params)\n",
    "    \n",
    "    # NEW API: Usiamo callbacks invece di early_stopping_rounds in .fit()\n",
    "    callbacks = [\n",
    "        early_stopping(stopping_rounds=100, verbose=False),\n",
    "        log_evaluation(period=0) # Zittisce l'output\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        model.fit(\n",
    "            X_opt_train, y_opt_train,\n",
    "            eval_set=[(X_opt_val, y_opt_val)],\n",
    "            eval_metric='rmse',\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Fallback CPU se GPU crasha o non presente\n",
    "        params['device'] = 'cpu'\n",
    "        model = LGBMRegressor(**params)\n",
    "        model.fit(\n",
    "            X_opt_train, y_opt_train,\n",
    "            eval_set=[(X_opt_val, y_opt_val)],\n",
    "            eval_metric='rmse',\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "\n",
    "    preds = model.predict(X_opt_val)\n",
    "    return np.sqrt(mean_squared_error(y_opt_val, preds))\n",
    "\n",
    "print(\"   -> Optimizing LightGBM...\")\n",
    "study_lgbm = optuna.create_study(direction='minimize')\n",
    "study_lgbm.optimize(objective_lgbm, n_trials=N_TRIALS)\n",
    "\n",
    "# Recupero i migliori parametri\n",
    "best_xgb_params = study_xgb.best_params\n",
    "best_xgb_params.update({'n_estimators': 2000, 'tree_method': 'hist', 'device': 'gpu', 'n_jobs': 1, 'random_state': 42})\n",
    "# Rimuoviamo early_stopping_rounds dai params per il fit finale (opzionale, ma pulito)\n",
    "if 'early_stopping_rounds' in best_xgb_params: del best_xgb_params['early_stopping_rounds']\n",
    "\n",
    "best_cat_params = study_cat.best_params\n",
    "best_cat_params.update({'iterations': 2000, 'task_type': 'GPU', 'devices': '0', 'verbose': 0, 'random_state': 42})\n",
    "\n",
    "best_lgbm_params = study_lgbm.best_params\n",
    "best_lgbm_params.update({'n_estimators': 2000, 'device': 'gpu', 'n_jobs': 1, 'verbosity': -1, 'random_state': 42})\n",
    "\n",
    "print(\"\\n--- Optuna Completato. Parametri migliori trovati. ---\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. SMART SOFT MODEL SELECTOR (Mixture of Experts)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n5. Costruzione Smart Soft Selector (Mixture of Experts)...\")\n",
    "\n",
    "class SmartSoftModelSelector(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, estimators, selector_model=None):\n",
    "        self.estimators = estimators # Lista di tuple ('nome', modello)\n",
    "        # Usiamo XGBClassifier su GPU come selettore per massima velocità\n",
    "        self.selector_model = selector_model if selector_model else XGBClassifier(\n",
    "            n_estimators=200, max_depth=6, learning_rate=0.05,\n",
    "            tree_method='hist', device='gpu', n_jobs=1, random_state=42\n",
    "        )\n",
    "        self.model_names = [name for name, _ in estimators]\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # A. Addestramento Modelli Base\n",
    "        print(\"   -> Training modelli base (Esperti) su tutto il Train Set...\")\n",
    "        self.fitted_estimators_ = []\n",
    "        for name, model in self.estimators:\n",
    "            # Nota: Qui non usiamo Early Stopping perché vorremmo usare tutto il train set.\n",
    "            # Usiamo i parametri ottimizzati da Optuna che sono robusti.\n",
    "            model.fit(X, y)\n",
    "            self.fitted_estimators_.append(model)\n",
    "            \n",
    "        # B. Generazione OOF Predictions (Chi sbaglia meno dove?)\n",
    "        print(\"   -> Generazione dati per il Gating Network...\")\n",
    "        errors = pd.DataFrame()\n",
    "        \n",
    "        # Nota: Usiamo n_jobs=1 in cross_val_predict perché i modelli usano GPU internamente\n",
    "        for name, model in self.estimators:\n",
    "            # cross_val_predict è essenziale per evitare leakage\n",
    "            oof_preds = cross_val_predict(model, X, y, cv=5, n_jobs=1)\n",
    "            errors[name] = np.abs(y - oof_preds) # Errore Assoluto\n",
    "            \n",
    "        # C. Creazione Target per il Selettore\n",
    "        y_best_model_idx = errors.idxmin(axis=1).apply(lambda x: self.model_names.index(x))\n",
    "        \n",
    "        # D. Addestramento Selettore (Gating Network)\n",
    "        print(\"   -> Training del Selettore (Gating Network)...\")\n",
    "        self.selector_model.fit(X, y_best_model_idx)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # A. Predizioni Base\n",
    "        base_preds = np.column_stack([model.predict(X) for model in self.fitted_estimators_])\n",
    "        # B. Pesi Soft\n",
    "        weights = self.selector_model.predict_proba(X)\n",
    "        # C. Media Ponderata Dinamica\n",
    "        final_pred = np.sum(base_preds * weights, axis=1)\n",
    "        return final_pred\n",
    "\n",
    "# Istanziamo i modelli base con i Best Params di Optuna\n",
    "estimators_list = [\n",
    "    ('xgb', XGBRegressor(**best_xgb_params)),\n",
    "    ('cat', CatBoostRegressor(**best_cat_params)),\n",
    "    ('lgbm', LGBMRegressor(**best_lgbm_params)) \n",
    "]\n",
    "\n",
    "# Creiamo e addestriamo il sistema MoE\n",
    "moe_model = SmartSoftModelSelector(estimators=estimators_list)\n",
    "moe_model.fit(X_train, y_train)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. AGGIUNTA DEL CORRETTORE DEI RESIDUI (Residual Learning)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n6. Training Correttore dei Residui...\")\n",
    "\n",
    "class ResidualCorrectedMoE(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, base_moe_model, corrector_model=None):\n",
    "        self.base_moe_model = base_moe_model\n",
    "        # Usiamo un CatBoost leggero come correttore\n",
    "        self.corrector_model = corrector_model if corrector_model else CatBoostRegressor(\n",
    "            iterations=500,          \n",
    "            depth=6, \n",
    "            learning_rate=0.03,\n",
    "            l2_leaf_reg=5,           \n",
    "            task_type='GPU', \n",
    "            devices='0',\n",
    "            verbose=0,\n",
    "            allow_writing_files=False,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # 2. Generiamo le predizioni OOF per calcolare i residui onesti\n",
    "        print(\"   -> Calcolo residui OOF (può richiedere tempo)...\")\n",
    "        oof_preds = cross_val_predict(self.base_moe_model, X, y, cv=5, n_jobs=1)\n",
    "        \n",
    "        # 3. Calcolo dell'Errore\n",
    "        residuals = y - oof_preds\n",
    "        \n",
    "        print(f\"      Media Residui: {residuals.mean():.4f}\")\n",
    "        print(f\"      Deviaz. Std Residui: {residuals.std():.4f}\")\n",
    "        \n",
    "        # 4. Addestriamo il Correttore\n",
    "        print(\"   -> Training del Correttore...\")\n",
    "        self.corrector_model.fit(X, residuals)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        base_pred = self.base_moe_model.predict(X)\n",
    "        correction = self.corrector_model.predict(X)\n",
    "        return base_pred + (1.0 * correction)\n",
    "\n",
    "# Costruzione Finale\n",
    "# Nota: passiamo il moe_model già addestrato, ma la classe ResidualCorrectedMoE\n",
    "# userà cross_val_predict che internamente farà cloni e fit su fold.\n",
    "final_system = ResidualCorrectedMoE(base_moe_model=moe_model)\n",
    "final_system.fit(X_train, y_train)\n",
    "\n",
    "# Valutazione\n",
    "y_pred_corrected = final_system.predict(X_test)\n",
    "\n",
    "final_rmse_corr = np.sqrt(mean_squared_error(y_test, y_pred_corrected))\n",
    "final_r2_corr = r2_score(y_test, y_pred_corrected)\n",
    "\n",
    "print(f\"\\n==========================================\")\n",
    "print(f\" RISULTATI FINALI OTTIMIZZATI\")\n",
    "print(f\"==========================================\")\n",
    "print(f\" RMSE: {final_rmse_corr:.5f}\")\n",
    "print(f\" R^2 : {final_r2_corr:.5f}\")\n",
    "print(f\"==========================================\")\n",
    "\n",
    "# Plot veloce\n",
    "corrections_test = final_system.corrector_model.predict(X_test)\n",
    "base_preds_test = final_system.base_moe_model.predict(X_test)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=base_preds_test, y=corrections_test, alpha=0.3)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.title(\"Bias Correction vs Price (Optimized Models)\")\n",
    "plt.xlabel(\"Predicted Price\")\n",
    "plt.ylabel(\"Correction\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rocm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
